{
  "v1": {
    "config": {},
    "helmValues": "# Default values for prometheus-operator.\n# This is a YAML-formatted file.\n# Declare variables to be passed into your templates.\n\n## Provide a name in place of prometheus-operator for `app:` labels\n##\nnameOverride: \"\"\n\n## Provide a name to substitute for the full names of resources\n##\nfullnameOverride: \"\"\n\n## Labels to apply to all resources\n##\ncommonLabels: {}\n# scmhash: abc123\n# myLabel: aakkmd\n\n## Create default rules for monitoring the cluster\n##\ndefaultRules:\n  create: true\n  rules:\n    alertmanager: true\n    etcd: true\n    general: true\n    k8s: true\n    kubeApiserver: true\n    kubePrometheusNodeAlerting: true\n    kubePrometheusNodeRecording: true\n    kubeScheduler: true\n    kubernetesAbsent: true\n    kubernetesApps: true\n    kubernetesResources: true\n    kubernetesStorage: true\n    kubernetesSystem: true\n    node: true\n    prometheusOperator: true\n    prometheus: true\n  ## Labels for default rules\n  labels: {}\n  ## Annotations for default rules\n  annotations: {}\n\n## Provide custom recording or alerting rules to be deployed into the cluster.\n##\nadditionalPrometheusRules: []\n#  - name: my-rule-file\n#    groups:\n#      - name: my_group\n#        rules:\n#        - record: my_record\n#          expr: 100 * my_record\n\n##\nglobal:\n  rbac:\n    create: true\n    pspEnabled: true\n  ## Reference to one or more secrets to be used when pulling images\n  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/\n  ##\n  imagePullSecrets: []\n  # - name: \"image-pull-secret\"\n\n## Configuration for alertmanager\n## ref: https://prometheus.io/docs/alerting/alertmanager/\n##\nalertmanager:\n  ## Deploy alertmanager\n  ##\n  enabled: true\n  ## Service account for Alertmanager to use.\n  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/\n  ##\n  serviceAccount:\n    create: true\n    name: \"\"\n\n  ## Configure pod disruption budgets for Alertmanager\n  ## ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb/#specifying-a-poddisruptionbudget\n  ## This configuration is immutable once created and will require the PDB to be deleted to be changed\n  ## https://github.com/kubernetes/kubernetes/issues/45398\n  ##\n  podDisruptionBudget:\n    enabled: false\n    minAvailable: 1\n    maxUnavailable: \"\"\n\n  ## Alertmanager configuration directives\n  ## ref: https://prometheus.io/docs/alerting/configuration/#configuration-file\n  ##      https://prometheus.io/webtools/alerting/routing-tree-editor/\n  ##\n  config:\n    global:\n      resolve_timeout: 5m\n    route:\n      group_by:\n      - job\n      group_wait: 30s\n      group_interval: 5m\n      repeat_interval: 12h\n      receiver: \"null\"\n      routes:\n      - match:\n          alertname: Watchdog\n        receiver: \"null\"\n    receivers:\n    - name: \"null\"\n\n  ## Alertmanager template files to format alerts\n  ## ref: https://prometheus.io/docs/alerting/notifications/\n  ##      https://prometheus.io/docs/alerting/notification_examples/\n  ##\n  templateFiles: {}\n  #\n  # An example template:\n  #   template_1.tmpl: |-\n  #       {{ define \"cluster\" }}{{ .ExternalURL | reReplaceAll \".*alertmanager\\\\.(.*)\" \"$1\" }}{{ end }}\n  #\n  #       {{ define \"slack.myorg.text\" }}\n  #       {{- $root := . -}}\n  #       {{ range .Alerts }}\n  #         *Alert:* {{ .Annotations.summary }} - `{{ .Labels.severity }}`\n  #         *Cluster:*  {{ template \"cluster\" $root }}\n  #         *Description:* {{ .Annotations.description }}\n  #         *Graph:* \u003c{{ .GeneratorURL }}|:chart_with_upwards_trend:\u003e\n  #         *Runbook:* \u003c{{ .Annotations.runbook }}|:spiral_note_pad:\u003e\n  #         *Details:*\n  #           {{ range .Labels.SortedPairs }} • *{{ .Name }}:* `{{ .Value }}`\n  #           {{ end }}\n\n  ingress:\n    enabled: false\n    annotations: {}\n\n    labels: {}\n\n    ## Hosts must be provided if Ingress is enabled.\n    ##\n    hosts: []\n      # - alertmanager.domain.com\n\n    ## Paths to use for ingress rules - one path should match the alertmanagerSpec.routePrefix\n    ##\n    paths: []\n    # - /\n\n    ## TLS configuration for Alertmanager Ingress\n    ## Secret must be manually created in the namespace\n    ##\n    tls: []\n    # - secretName: alertmanager-general-tls\n    #   hosts:\n    #   - alertmanager.example.com\n\n  ## Configuration for Alertmanager service\n  ##\n  service:\n    annotations: {}\n    labels: {}\n    clusterIP: \"\"\n\n  ## Port to expose on each node\n  ## Only used if service.type is 'NodePort'\n  ##\n    nodePort: 30903\n  ## List of IP addresses at which the Prometheus server service is available\n  ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips\n  ##\n    externalIPs: []\n    loadBalancerIP: \"\"\n    loadBalancerSourceRanges: []\n    ## Service type\n    ##\n    type: ClusterIP\n  ## If true, create a serviceMonitor for alertmanager\n  ##\n  serviceMonitor:\n    ## Scrape interval. If not set, the Prometheus default scrape interval is used.\n    ##\n    interval: \"\"\n    selfMonitor: true\n  ## Settings affecting alertmanagerSpec\n  ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#alertmanagerspec\n  ##\n  alertmanagerSpec:\n    ## Standard object’s metadata. More info: https://github.com/kubernetes/community/blob/master/contributors/devel/api-conventions.md#metadata\n    ## Metadata Labels and Annotations gets propagated to the Alertmanager pods.\n    ##\n    podMetadata: {}\n\n    ## Image of Alertmanager\n    ##\n    image:\n      repository: quay.io/prometheus/alertmanager\n      tag: v0.16.1\n    ## Secrets is a list of Secrets in the same namespace as the Alertmanager object, which shall be mounted into the\n    ## Alertmanager Pods. The Secrets are mounted into /etc/alertmanager/secrets/.\n    ##\n    secrets: []\n\n    ## ConfigMaps is a list of ConfigMaps in the same namespace as the Alertmanager object, which shall be mounted into the Alertmanager Pods.\n    ## The ConfigMaps are mounted into /etc/alertmanager/configmaps/.\n    ##\n    configMaps: []\n\n    ## Log level for Alertmanager to be configured with.\n    ##\n    logLevel: info\n    ## Size is the expected size of the alertmanager cluster. The controller will eventually make the size of the\n    ## running cluster equal to the expected size.\n    replicas: 1\n    ## Time duration Alertmanager shall retain data for. Default is '120h', and must match the regular expression\n    ## [0-9]+(ms|s|m|h) (milliseconds seconds minutes hours).\n    ##\n    retention: 120h\n    ## Storage is the definition of how storage will be used by the Alertmanager instances.\n    ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/user-guides/storage.md\n    ##\n    storage: {}\n    # volumeClaimTemplate:\n    #   spec:\n    #     storageClassName: gluster\n    #     accessModes: [\"ReadWriteOnce\"]\n    #     resources:\n    #       requests:\n    #         storage: 50Gi\n    #   selector: {}\n\n\n    ## \tThe external URL the Alertmanager instances will be available under. This is necessary to generate correct URLs. This is necessary if Alertmanager is not served from root of a DNS name.\tstring\tfalse\n    ##\n    externalUrl: null\n    routePrefix: /\n    ## If set to true all actions on the underlying managed objects are not going to be performed, except for delete actions.\n    ##\n    paused: false\n    ## Define which Nodes the Pods are scheduled on.\n    ## ref: https://kubernetes.io/docs/user-guide/node-selection/\n    ##\n    nodeSelector: {}\n\n    ## Define resources requests and limits for single Pods.\n    ## ref: https://kubernetes.io/docs/user-guide/compute-resources/\n    ##\n    resources: {}\n    # requests:\n    #   memory: 400Mi\n\n    ## Pod anti-affinity can prevent the scheduler from placing Prometheus replicas on the same node.\n    ## The default value \"soft\" means that the scheduler should *prefer* to not schedule two replica pods onto the same node but no guarantee is provided.\n    ## The value \"hard\" means that the scheduler is *required* to not schedule two replica pods onto the same node.\n    ## The value \"\" will disable pod anti-affinity so that no anti-affinity rules will be configured.\n    ##\n    podAntiAffinity: \"\"\n\n    ## If anti-affinity is enabled sets the topologyKey to use for anti-affinity.\n    ## This can be changed to, for example, failure-domain.beta.kubernetes.io/zone\n    ##\n    podAntiAffinityTopologyKey: kubernetes.io/hostname\n    ## If specified, the pod's tolerations.\n    ## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/\n    ##\n    tolerations: []\n    # - key: \"key\"\n    #   operator: \"Equal\"\n    #   value: \"value\"\n    #   effect: \"NoSchedule\"\n\n    ## SecurityContext holds pod-level security attributes and common container settings.\n    ## This defaults to non root user with uid 1000 and gid 2000.\t*v1.PodSecurityContext\tfalse\n    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/\n    ##\n    securityContext:\n      runAsNonRoot: true\n      runAsUser: 1000\n      fsGroup: 2000\n    ## ListenLocal makes the Alertmanager server listen on loopback, so that it does not bind against the Pod IP.\n    ## Note this is only for the Alertmanager UI, not the gossip communication.\n    ##\n    listenLocal: false\n    ## Containers allows injecting additional containers. This is meant to allow adding an authentication proxy to an Alertmanager pod.\n    ##\n    containers: []\n\n    ## Priority class assigned to the Pods\n    ##\n    priorityClassName: \"\"\n\n    ## AdditionalPeers allows injecting a set of additional Alertmanagers to peer with to form a highly available cluster.\n    ##\n    additionalPeers: []\n\n## Using default values from https://github.com/helm/charts/blob/master/stable/grafana/values.yaml\n##\ngrafana:\n  enabled: true\n  ## Deploy default dashboards.\n  ##\n  defaultDashboardsEnabled: true\n  adminPassword: prom-operator\n  ingress:\n    ## If true, Grafana Ingress will be created\n    ##\n    enabled: false\n    ## Annotations for Grafana Ingress\n    ##\n    annotations: {}\n      # kubernetes.io/ingress.class: nginx\n      # kubernetes.io/tls-acme: \"true\"\n\n    ## Labels to be added to the Ingress\n    ##\n    labels: {}\n\n    ## Hostnames.\n    ## Must be provided if Ingress is enable.\n    ##\n    # hosts:\n    #   - grafana.domain.com\n    hosts: []\n\n    ## Path for grafana ingress\n    path: /\n    ## TLS configuration for grafana Ingress\n    ## Secret must be manually created in the namespace\n    ##\n    tls: []\n    # - secretName: grafana-general-tls\n    #   hosts:\n    #   - grafana.example.com\n\n  sidecar:\n    dashboards:\n      enabled: true\n      label: grafana_dashboard\n    datasources:\n      enabled: true\n      defaultDatasourceEnabled: true\n      label: grafana_datasource\n  extraConfigmapMounts: []\n  # - name: certs-configmap\n  #   mountPath: /etc/grafana/ssl/\n  #   configMap: certs-configmap\n  #   readOnly: true\n\n  ## Configure additional grafana datasources\n  ## ref: http://docs.grafana.org/administration/provisioning/#datasources\n  additionalDataSources: []\n  # - name: prometheus-sample\n  #   access: proxy\n  #   basicAuth: true\n  #   basicAuthPassword: pass\n  #   basicAuthUser: daco\n  #   editable: false\n  #   jsonData:\n  #       tlsSkipVerify: true\n  #   orgId: 1\n  #   type: prometheus\n  #   url: https://prometheus.svc:9090\n  #   version: 1\n\n  ## If true, create a serviceMonitor for grafana\n  ##\n  serviceMonitor:\n    ## Scrape interval. If not set, the Prometheus default scrape interval is used.\n    ##\n    interval: \"\"\n    selfMonitor: true\n## Component scraping the kube api server\n##\nkubeApiServer:\n  enabled: false\n  tlsConfig:\n    serverName: kubernetes\n    insecureSkipVerify: false\n  ## If your API endpoint address is not reachable (as in AKS) you can replace it with the kubernetes service\n  ##\n  relabelings: []\n  # - sourceLabels:\n  #     - __meta_kubernetes_namespace\n  #     - __meta_kubernetes_service_name\n  #     - __meta_kubernetes_endpoint_port_name\n  #   action: keep\n  #   regex: default;kubernetes;https\n  # - targetLabel: __address__\n  #   replacement: kubernetes.default.svc:443\n\n  serviceMonitor:\n    ## Scrape interval. If not set, the Prometheus default scrape interval is used.\n    ##\n    interval: \"\"\n    jobLabel: component\n    selector:\n      matchLabels:\n        component: apiserver\n        provider: kubernetes\n## Component scraping the kubelet and kubelet-hosted cAdvisor\n##\nkubelet:\n  enabled: false\n  namespace: kube-system\n  serviceMonitor:\n    ## Scrape interval. If not set, the Prometheus default scrape interval is used.\n    ##\n    interval: \"\"\n\n    ## Enable scraping the kubelet over https. For requirements to enable this see\n    ## https://github.com/coreos/prometheus-operator/issues/926\n    ##\n    https: true\n    ## Metric relabellings to apply to samples before ingestion\n    ##\n    cAdvisorMetricRelabelings: []\n    # - sourceLabels: [__name__, image]\n    #   separator: ;\n    #   regex: container_([a-z_]+);\n    #   replacement: $1\n    #   action: drop\n    # - sourceLabels: [__name__]\n    #   separator: ;\n    #   regex: container_(network_tcp_usage_total|network_udp_usage_total|tasks_state|cpu_load_average_10s)\n    #   replacement: $1\n    #   action: drop\n\n\n## Component scraping the kube controller manager\n##\nkubeControllerManager:\n  enabled: false\n  ## If your kube controller manager is not deployed as a pod, specify IPs it can be found on\n  ##\n  endpoints: []\n  # - 10.141.4.22\n  # - 10.141.4.23\n  # - 10.141.4.24\n\n  ## If using kubeControllerManager.endpoints only the port and targetPort are used\n  ##\n  service:\n    port: 10252\n    targetPort: 10252\n    selector:\n      component: kube-controller-manager\n  serviceMonitor:\n    ## Scrape interval. If not set, the Prometheus default scrape interval is used.\n    ##\n    interval: \"\"\n\n    ## Enable scraping kube-controller-manager over https.\n    ## Requires proper certs (not self-signed) and delegated authentication/authorization checks\n    ##\n    https: false\n## Component scraping coreDns. Use either this or kubeDns\n##\ncoreDns:\n  enabled: false\n  service:\n    port: 9153\n    targetPort: 9153\n    selector:\n      k8s-app: kube-dns\n  serviceMonitor:\n    ## Scrape interval. If not set, the Prometheus default scrape interval is used.\n    ##\n    interval: \"\"\n\n## Component scraping kubeDns. Use either this or coreDns\n##\nkubeDns:\n  enabled: false\n  service:\n    selector:\n      k8s-app: kube-dns\n  serviceMonitor:\n    ## Scrape interval. If not set, the Prometheus default scrape interval is used.\n    ##\n    interval: \"\"\n## Component scraping etcd\n##\nkubeEtcd:\n  enabled: false\n  ## If your etcd is not deployed as a pod, specify IPs it can be found on\n  ##\n  endpoints: []\n  # - 10.141.4.22\n  # - 10.141.4.23\n  # - 10.141.4.24\n\n  ## Etcd service. If using kubeEtcd.endpoints only the port and targetPort are used\n  ##\n  service:\n    port: 2379\n    targetPort: 2379\n    selector:\n      component: etcd\n  ## Configure secure access to the etcd cluster by loading a secret into prometheus and\n  ## specifying security configuration below. For example, with a secret named etcd-client-cert\n  ##\n  ## serviceMonitor:\n  ##   scheme: https\n  ##   insecureSkipVerify: false\n  ##   serverName: localhost\n  ##   caFile: /etc/prometheus/secrets/etcd-client-cert/etcd-ca\n  ##   certFile: /etc/prometheus/secrets/etcd-client-cert/etcd-client\n  ##   keyFile: /etc/prometheus/secrets/etcd-client-cert/etcd-client-key\n  ##\n  serviceMonitor:\n    ## Scrape interval. If not set, the Prometheus default scrape interval is used.\n    ##\n    interval: \"\"\n    scheme: http\n    insecureSkipVerify: false\n    serverName: \"\"\n    caFile: \"\"\n    certFile: \"\"\n    keyFile: \"\"\n\n\n## Component scraping kube scheduler\n##\nkubeScheduler:\n  enabled: false\n  ## If your kube scheduler is not deployed as a pod, specify IPs it can be found on\n  ##\n  endpoints: []\n  # - 10.141.4.22\n  # - 10.141.4.23\n  # - 10.141.4.24\n\n  ## If using kubeScheduler.endpoints only the port and targetPort are used\n  ##\n  service:\n    port: 10251\n    targetPort: 10251\n    selector:\n      component: kube-scheduler\n  serviceMonitor:\n    ## Scrape interval. If not set, the Prometheus default scrape interval is used.\n    ##\n    interval: \"\"\n    ## Enable scraping kube-controller-manager over https.\n    ## Requires proper certs (not self-signed) and delegated authentication/authorization checks\n    ##\n    https: false\n## Component scraping kube state metrics\n##\nkubeStateMetrics:\n  enabled: true\n  serviceMonitor:\n    ## Scrape interval. If not set, the Prometheus default scrape interval is used.\n    ##\n    interval: \"\"\n\n    ## \tmetric relabel configs to apply to samples before ingestion.\n    ##\n    metricRelabelings: []\n    # - action: keep\n    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'\n    #   sourceLabels: [__name__]\n\n## Configuration for kube-state-metrics subchart\n##\nkube-state-metrics:\n  rbac:\n    create: true\n  podSecurityPolicy:\n    enabled: true\n## Deploy node exporter as a daemonset to all nodes\n##\nnodeExporter:\n  enabled: true\n  ## Use the value configured in prometheus-node-exporter.podLabels\n  ##\n  jobLabel: jobLabel\n  serviceMonitor:\n    ## Scrape interval. If not set, the Prometheus default scrape interval is used.\n    ##\n    interval: \"\"\n\n    ## \tmetric relabel configs to apply to samples before ingestion.\n    ##\n    metricRelabelings: []\n    # - sourceLabels: [__name__]\n    #   separator: ;\n    #   regex: ^node_mountstats_nfs_(event|operations|transport)_.+\n    #   replacement: $1\n    #   action: drop\n\n## Configuration for prometheus-node-exporter subchart\n##\nprometheus-node-exporter:\n  podLabels:\n    ## Add the 'node-exporter' label to be used by serviceMonitor to match standard common usage in rules and grafana dashboards\n    ##\n    jobLabel: node-exporter\n  extraArgs:\n  - --collector.filesystem.ignored-mount-points=^/(dev|proc|sys|var/lib/docker/.+)($|/)\n  - --collector.filesystem.ignored-fs-types=^(autofs|binfmt_misc|cgroup|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|mqueue|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|sysfs|tracefs)$\n## Manages Prometheus and Alertmanager components\n##\nprometheusOperator:\n  enabled: true\n  ## Service account for Alertmanager to use.\n  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/\n  ##\n  serviceAccount:\n    create: true\n    name: \"\"\n\n  ## Configuration for Prometheus operator service\n  ##\n  service:\n    annotations: {}\n    labels: {}\n    clusterIP: \"\"\n\n  ## Port to expose on each node\n  ## Only used if service.type is 'NodePort'\n  ##\n    nodePort: 30080\n  ## Additional ports to open for Prometheus service\n  ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#multi-port-services\n  ##\n    additionalPorts: []\n    #  - name: thanos-cluster\n    #    port: 10900\n    #    nodePort: 30111\n\n  ## Loadbalancer IP\n  ## Only use if service.type is \"loadbalancer\"\n  ##\n    loadBalancerIP: \"\"\n    loadBalancerSourceRanges: []\n\n  ## Service type\n  ## NodepPort, ClusterIP, loadbalancer\n  ##\n    type: ClusterIP\n    ## List of IP addresses at which the Prometheus server service is available\n    ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips\n    ##\n    externalIPs: []\n\n  ## Deploy CRDs used by Prometheus Operator.\n  ##\n  createCustomResource: true\n  ## Customize CRDs API Group\n  crdApiGroup: monitoring.coreos.com\n  ## Attempt to clean up CRDs created by Prometheus Operator.\n  ##\n  cleanupCustomResource: false\n  ## Labels to add to the operator pod\n  ##\n  podLabels: {}\n\n  ## Annotations to add to the operator pod\n  ##\n  podAnnotations: {}\n\n  ## Assign a PriorityClassName to pods if set\n  # priorityClassName: \"\"\n\n  ## Define Log Format\n  # Use logfmt (default) or json-formatted logging\n  # logFormat: logfmt\n\n  ## Decrease log verbosity to errors only\n  # logLevel: error\n\n  ## If true, the operator will create and maintain a service for scraping kubelets\n  ## ref: https://github.com/coreos/prometheus-operator/blob/master/helm/prometheus-operator/README.md\n  ##\n  kubeletService:\n    enabled: true\n    namespace: kube-system\n  ## Create a servicemonitor for the operator\n  ##\n  serviceMonitor:\n    ## Scrape interval. If not set, the Prometheus default scrape interval is used.\n    ##\n    interval: \"\"\n    selfMonitor: true\n  ## Resource limits \u0026 requests\n  ##\n  resources: {}\n  # limits:\n  #   cpu: 200m\n  #   memory: 200Mi\n  # requests:\n  #   cpu: 100m\n  #   memory: 100Mi\n\n  ## Define which Nodes the Pods are scheduled on.\n  ## ref: https://kubernetes.io/docs/user-guide/node-selection/\n  ##\n  nodeSelector: {}\n\n  ## Tolerations for use with node taints\n  ## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/\n  ##\n  tolerations: []\n  # - key: \"key\"\n  #   operator: \"Equal\"\n  #   value: \"value\"\n  #   effect: \"NoSchedule\"\n\n  ## Assign the prometheus operator to run on specific nodes\n  ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/\n  ##\n  affinity: {}\n  # requiredDuringSchedulingIgnoredDuringExecution:\n  #   nodeSelectorTerms:\n  #   - matchExpressions:\n  #     - key: kubernetes.io/e2e-az-name\n  #       operator: In\n  #       values:\n  #       - e2e-az1\n  #       - e2e-az2\n\n  securityContext:\n    runAsNonRoot: true\n    runAsUser: 65534\n  ## Prometheus-operator image\n  ##\n  image:\n    repository: quay.io/coreos/prometheus-operator\n    tag: v0.29.0\n    pullPolicy: IfNotPresent\n  ## Configmap-reload image to use for reloading configmaps\n  ##\n  configmapReloadImage:\n    repository: quay.io/coreos/configmap-reload\n    tag: v0.0.1\n  ## Prometheus-config-reloader image to use for config and rule reloading\n  ##\n  prometheusConfigReloaderImage:\n    repository: quay.io/coreos/prometheus-config-reloader\n    tag: v0.29.0\n  ## Set the prometheus config reloader side-car CPU limit. If unset, uses the prometheus-operator project default\n  ##\n  # configReloaderCpu: 100m\n\n  ## Set the prometheus config reloader side-car memory limit. If unset, uses the prometheus-operator project default\n  ##\n  # configReloaderMemory: 25Mi\n\n  ## Hyperkube image to use when cleaning up\n  ##\n  hyperkubeImage:\n    repository: k8s.gcr.io/hyperkube\n    tag: v1.12.1\n    pullPolicy: IfNotPresent\n## Deploy a Prometheus instance\n##\nprometheus:\n  enabled: true\n  ## Service account for Prometheuses to use.\n  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/\n  ##\n  serviceAccount:\n    create: true\n    name: \"\"\n\n  ## Configuration for Prometheus service\n  ##\n  service:\n    annotations: {}\n    labels: {}\n    clusterIP: \"\"\n\n\n    ## To be used with a proxy extraContainer port\n    targetPort: 9090\n    ## List of IP addresses at which the Prometheus server service is available\n    ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips\n    ##\n    externalIPs: []\n\n    ## Port to expose on each node\n    ## Only used if service.type is 'NodePort'\n    ##\n    nodePort: 30090\n    ## Loadbalancer IP\n    ## Only use if service.type is \"loadbalancer\"\n    loadBalancerIP: \"\"\n    loadBalancerSourceRanges: []\n    ## Service type\n    ##\n    type: ClusterIP\n    sessionAffinity: \"\"\n\n  rbac:\n    ## Create role bindings in the specified namespaces, to allow Prometheus monitoring\n    ## a role binding in the release namespace will always be created.\n    ##\n    roleNamespaces:\n    - kube-system\n  ## Configure pod disruption budgets for Prometheus\n  ## ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb/#specifying-a-poddisruptionbudget\n  ## This configuration is immutable once created and will require the PDB to be deleted to be changed\n  ## https://github.com/kubernetes/kubernetes/issues/45398\n  ##\n  podDisruptionBudget:\n    enabled: false\n    minAvailable: 1\n    maxUnavailable: \"\"\n\n  ingress:\n    enabled: false\n    annotations: {}\n    labels: {}\n\n    ## Hostnames.\n    ## Must be provided if Ingress is enabled.\n    ##\n    # hosts:\n    #   - prometheus.domain.com\n    hosts: []\n\n    ## Paths to use for ingress rules - one path should match the prometheusSpec.routePrefix\n    ##\n    paths: []\n    # - /\n\n    ## TLS configuration for Prometheus Ingress\n    ## Secret must be manually created in the namespace\n    ##\n    tls: []\n      # - secretName: prometheus-general-tls\n      #   hosts:\n      #     - prometheus.example.com\n\n  serviceMonitor:\n    ## Scrape interval. If not set, the Prometheus default scrape interval is used.\n    ##\n    interval: \"\"\n    selfMonitor: true\n  ## Settings affecting prometheusSpec\n  ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#prometheusspec\n  ##\n  prometheusSpec:\n    ## Interval between consecutive scrapes.\n    ##\n    scrapeInterval: \"\"\n\n    ## Interval between consecutive evaluations.\n    ##\n    evaluationInterval: \"\"\n\n    ## ListenLocal makes the Prometheus server listen on loopback, so that it does not bind against the Pod IP.\n    ##\n    listenLocal: false\n    ## Image of Prometheus.\n    ##\n    image:\n      repository: quay.io/prometheus/prometheus\n      tag: v2.7.1\n    ## Tolerations for use with node taints\n    ## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/\n    ##\n    tolerations: []\n    #  - key: \"key\"\n    #    operator: \"Equal\"\n    #    value: \"value\"\n    #    effect: \"NoSchedule\"\n\n    ## Alertmanagers to which alerts will be sent\n    ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#alertmanagerendpoints\n    ##\n    ## Default configuration will connect to the alertmanager deployed as part of this release\n    ##\n    alertingEndpoints: []\n    # - name: \"\"\n    #   namespace: \"\"\n    #   port: http\n    #   scheme: http\n\n    ## External labels to add to any time series or alerts when communicating with external systems\n    ##\n    externalLabels: {}\n\n    ## External URL at which Prometheus will be reachable.\n    ##\n    externalUrl: \"\"\n\n    ## Define which Nodes the Pods are scheduled on.\n    ## ref: https://kubernetes.io/docs/user-guide/node-selection/\n    ##\n    nodeSelector: {}\n\n    ## Secrets is a list of Secrets in the same namespace as the Prometheus object, which shall be mounted into the Prometheus Pods.\n    ## The Secrets are mounted into /etc/prometheus/secrets/. Secrets changes after initial creation of a Prometheus object are not\n    ## reflected in the running Pods. To change the secrets mounted into the Prometheus Pods, the object must be deleted and recreated\n    ## with the new list of secrets.\n    ##\n    secrets: []\n\n    ## ConfigMaps is a list of ConfigMaps in the same namespace as the Prometheus object, which shall be mounted into the Prometheus Pods.\n    ## The ConfigMaps are mounted into /etc/prometheus/configmaps/.\n    ##\n    configMaps: []\n\n    ## QuerySpec defines the query command line flags when starting Prometheus.\n    ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#queryspec\n    ##\n    query: {}\n\n    ## Namespaces to be selected for PrometheusRules discovery.\n    ## If nil, select own namespace. Namespaces to be selected for ServiceMonitor discovery.\n    ## See https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#namespaceselector for usage\n    ##\n    ruleNamespaceSelector: {}\n\n    ## If true, a nil or {} value for prometheus.prometheusSpec.ruleSelector will cause the\n    ## prometheus resource to be created with selectors based on values in the helm deployment,\n    ## which will also match the PrometheusRule resources created\n    ##\n    ruleSelectorNilUsesHelmValues: false\n    ## PrometheusRules to be selected for target discovery.\n    ## If {}, select all ServiceMonitors\n    ##\n    ruleSelector:\n      matchLabels:\n        app: discovery\n    ## Example which select all prometheusrules resources\n    ## with label \"prometheus\" with values any of \"example-rules\" or \"example-rules-2\"\n    # ruleSelector:\n    #   matchExpressions:\n    #     - key: prometheus\n    #       operator: In\n    #       values:\n    #         - example-rules\n    #         - example-rules-2\n    #\n    ## Example which select all prometheusrules resources with label \"role\" set to \"example-rules\"\n    # ruleSelector:\n    #   matchLabels:\n    #     role: example-rules\n\n    ## If true, a nil or {} value for prometheus.prometheusSpec.serviceMonitorSelector will cause the\n    ## prometheus resource to be created with selectors based on values in the helm deployment,\n    ## which will also match the servicemonitors created\n    ##\n    serviceMonitorSelectorNilUsesHelmValues: false\n    ## ServiceMonitors to be selected for target discovery.\n    ## If {}, select all ServiceMonitors\n    ##\n    serviceMonitorSelector:\n      matchLabels:\n        app: discovery\n    ## Example which selects ServiceMonitors with label \"prometheus\" set to \"somelabel\"\n    # serviceMonitorSelector:\n    #   matchLabels:\n    #     prometheus: somelabel\n\n    ## Namespaces to be selected for ServiceMonitor discovery.\n    ## See https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#namespaceselector for usage\n    ##\n    serviceMonitorNamespaceSelector: {}\n\n    ## How long to retain metrics\n    ##\n    retention: 10d\n    ## If true, the Operator won't process any Prometheus configuration changes\n    ##\n    paused: false\n    ## Number of Prometheus replicas desired\n    ##\n    replicas: 1\n    ## Log level for Prometheus be configured in\n    ##\n    logLevel: info\n    ## Prefix used to register routes, overriding externalUrl route.\n    ## Useful for proxies that rewrite URLs.\n    ##\n    routePrefix: /\n    ## Standard object’s metadata. More info: https://github.com/kubernetes/community/blob/master/contributors/devel/api-conventions.md#metadata\n    ## Metadata Labels and Annotations gets propagated to the prometheus pods.\n    ##\n    podMetadata: {}\n    # labels:\n    #   app: prometheus\n    #   k8s-app: prometheus\n\n    ## Pod anti-affinity can prevent the scheduler from placing Prometheus replicas on the same node.\n    ## The default value \"soft\" means that the scheduler should *prefer* to not schedule two replica pods onto the same node but no guarantee is provided.\n    ## The value \"hard\" means that the scheduler is *required* to not schedule two replica pods onto the same node.\n    ## The value \"\" will disable pod anti-affinity so that no anti-affinity rules will be configured.\n    podAntiAffinity: \"\"\n\n    ## If anti-affinity is enabled sets the topologyKey to use for anti-affinity.\n    ## This can be changed to, for example, failure-domain.beta.kubernetes.io/zone\n    ##\n    podAntiAffinityTopologyKey: kubernetes.io/hostname\n    ## The remote_read spec configuration for Prometheus.\n    ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#remotereadspec\n    remoteRead: []\n    # - url: http://remote1/read\n\n    ## The remote_write spec configuration for Prometheus.\n    ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#remotewritespec\n    remoteWrite: []\n    # - url: http://remote1/push\n\n    ## Resource limits \u0026 requests\n    ##\n    resources: {}\n    # requests:\n    #   memory: 400Mi\n\n    ## Prometheus StorageSpec for persistent data\n    ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/user-guides/storage.md\n    ##\n    storageSpec: {}\n    #  volumeClaimTemplate:\n    #    spec:\n    #      storageClassName: gluster\n    #      accessModes: [\"ReadWriteOnce\"]\n    #      resources:\n    #        requests:\n    #          storage: 50Gi\n    #    selector: {}\n\n    ## AdditionalScrapeConfigs allows specifying additional Prometheus scrape configurations. Scrape configurations\n    ## are appended to the configurations generated by the Prometheus Operator. Job configurations must have the form\n    ## as specified in the official Prometheus documentation:\n    ## https://prometheus.io/docs/prometheus/latest/configuration/configuration/#\u003cscrape_config\u003e. As scrape configs are\n    ## appended, the user is responsible to make sure it is valid. Note that using this feature may expose the possibility\n    ## to break upgrades of Prometheus. It is advised to review Prometheus release notes to ensure that no incompatible\n    ## scrape configs are going to break Prometheus after the upgrade.\n    ##\n    ## The scrape configuraiton example below will find master nodes, provided they have the name .*mst.*, relabel the\n    ## port to 2379 and allow etcd scraping provided it is running on all Kubernetes master nodes\n    ##\n    additionalScrapeConfigs: []\n    # - job_name: kube-etcd\n    #   kubernetes_sd_configs:\n    #     - role: node\n    #   scheme: https\n    #   tls_config:\n    #     ca_file:   /etc/prometheus/secrets/etcd-client-cert/etcd-ca\n    #     cert_file: /etc/prometheus/secrets/etcd-client-cert/etcd-client\n    #     key_file:  /etc/prometheus/secrets/etcd-client-cert/etcd-client-key\n    #   relabel_configs:\n    #   - action: labelmap\n    #     regex: __meta_kubernetes_node_label_(.+)\n    #   - source_labels: [__address__]\n    #     action: replace\n    #     target_label: __address__\n    #     regex: ([^:;]+):(\\d+)\n    #     replacement: ${1}:2379\n    #   - source_labels: [__meta_kubernetes_node_name]\n    #     action: keep\n    #     regex: .*mst.*\n    #   - source_labels: [__meta_kubernetes_node_name]\n    #     action: replace\n    #     target_label: node\n    #     regex: (.*)\n    #     replacement: ${1}\n    #   metric_relabel_configs:\n    #   - regex: (kubernetes_io_hostname|failure_domain_beta_kubernetes_io_region|beta_kubernetes_io_os|beta_kubernetes_io_arch|beta_kubernetes_io_instance_type|failure_domain_beta_kubernetes_io_zone)\n    #     action: labeldrop\n\n\n    ## AdditionalAlertManagerConfigs allows for manual configuration of alertmanager jobs in the form as specified\n    ## in the official Prometheus documentation https://prometheus.io/docs/prometheus/latest/configuration/configuration/#\u003calertmanager_config\u003e.\n    ## AlertManager configurations specified are appended to the configurations generated by the Prometheus Operator.\n    ## As AlertManager configs are appended, the user is responsible to make sure it is valid. Note that using this\n    ## feature may expose the possibility to break upgrades of Prometheus. It is advised to review Prometheus release\n    ## notes to ensure that no incompatible AlertManager configs are going to break Prometheus after the upgrade.\n    ##\n    additionalAlertManagerConfigs: []\n    # - consul_sd_configs:\n    #   - server: consul.dev.test:8500\n    #     scheme: http\n    #     datacenter: dev\n    #     tag_separator: ','\n    #     services:\n    #       - metrics-prometheus-alertmanager\n\n    ## AdditionalAlertRelabelConfigs allows specifying Prometheus alert relabel configurations. Alert relabel configurations specified are appended\n    ## to the configurations generated by the Prometheus Operator. Alert relabel configurations specified must have the form as specified in the\n    ## official Prometheus documentation: https://prometheus.io/docs/prometheus/latest/configuration/configuration/#alert_relabel_configs.\n    ## As alert relabel configs are appended, the user is responsible to make sure it is valid. Note that using this feature may expose the\n    ## possibility to break upgrades of Prometheus. It is advised to review Prometheus release notes to ensure that no incompatible alert relabel\n    ## configs are going to break Prometheus after the upgrade.\n    ##\n    additionalAlertRelabelConfigs: []\n    # - separator: ;\n    #   regex: prometheus_replica\n    #   replacement: $1\n    #   action: labeldrop\n\n    ## SecurityContext holds pod-level security attributes and common container settings.\n    ## This defaults to non root user with uid 1000 and gid 2000.\n    ## https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md\n    ##\n    securityContext:\n      runAsNonRoot: true\n      runAsUser: 1000\n      fsGroup: 2000\n    ## \tPriority class assigned to the Pods\n    ##\n    priorityClassName: \"\"\n\n    ## Thanos configuration allows configuring various aspects of a Prometheus server in a Thanos environment.\n    ## This section is experimental, it may change significantly without deprecation notice in any release.\n    ## This is experimental and may change significantly without backward compatibility in any release.\n    ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#thanosspec\n    ##\n    thanos: {}\n\n    ## Containers allows injecting additional containers. This is meant to allow adding an authentication proxy to a Prometheus pod.\n    ##  if using proxy extraContainer  update targetPort with proxy container port\n    containers: []\n\n    ## Enable additional scrape configs that are managed externally to this chart. Note that the prometheus\n    ## will fail to provision if the correct secret does not exist.\n    ##\n    additionalScrapeConfigsExternal: false\n  additionalServiceMonitors: []\n  ## Name of the ServiceMonitor to create\n  ##\n  # - name: \"\"\n\n    ## Additional labels to set used for the ServiceMonitorSelector. Together with standard labels from\n    ## the chart\n    ##\n    # additionalLabels: {}\n\n    ## Service label for use in assembling a job name of the form \u003clabel value\u003e-\u003cport\u003e\n    ## If no label is specified, the service name is used.\n    ##\n    # jobLabel: \"\"\n\n    ## Label selector for services to which this ServiceMonitor applies\n    ##\n    # selector: {}\n\n    ## Namespaces from which services are selected\n    ##\n    # namespaceSelector:\n      ## Match any namespace\n      ##\n      # any: false\n\n      ## Explicit list of namespace names to select\n      ##\n      # matchNames: []\n\n    ## Endpoints of the selected service to be monitored\n    ##\n    # endpoints: []\n      ## Name of the endpoint's service port\n      ## Mutually exclusive with targetPort\n      # - port: \"\"\n\n      ## Name or number of the endpoint's target port\n      ## Mutually exclusive with port\n      # - targetPort: \"\"\n\n      ## File containing bearer token to be used when scraping targets\n      ##\n      #   bearerTokenFile: \"\"\n\n      ## Interval at which metrics should be scraped\n      ##\n      #   interval: 30s\n\n      ## HTTP path to scrape for metrics\n      ##\n      #   path: /metrics\n\n      ## HTTP scheme to use for scraping\n      ##\n      #   scheme: http\n\n      ## TLS configuration to use when scraping the endpoint\n      ##\n      #   tlsConfig:\n\n          ## Path to the CA file\n          ##\n          # caFile: \"\"\n\n          ## Path to client certificate file\n          ##\n          # certFile: \"\"\n\n          ## Skip certificate verification\n          ##\n          # insecureSkipVerify: false\n\n          ## Path to client key file\n          ##\n          # keyFile: \"\"\n\n          ## Server name used to verify host name\n          ##\n          # serverName: \"\"\n",
    "releaseName": "prometheus-operator",
    "helmValuesDefaults": "# Default values for prometheus-operator.\n# This is a YAML-formatted file.\n# Declare variables to be passed into your templates.\n\n## Provide a name in place of prometheus-operator for `app:` labels\n##\nnameOverride: \"\"\n\n## Provide a name to substitute for the full names of resources\n##\nfullnameOverride: \"\"\n\n## Labels to apply to all resources\n##\ncommonLabels: {}\n# scmhash: abc123\n# myLabel: aakkmd\n\n## Create default rules for monitoring the cluster\n##\ndefaultRules:\n  create: true\n  rules:\n    alertmanager: true\n    etcd: true\n    general: true\n    k8s: true\n    kubeApiserver: true\n    kubePrometheusNodeAlerting: true\n    kubePrometheusNodeRecording: true\n    kubeScheduler: true\n    kubernetesAbsent: true\n    kubernetesApps: true\n    kubernetesResources: true\n    kubernetesStorage: true\n    kubernetesSystem: true\n    node: true\n    prometheusOperator: true\n    prometheus: true\n  ## Labels for default rules\n  labels: {}\n  ## Annotations for default rules\n  annotations: {}\n\n## Provide custom recording or alerting rules to be deployed into the cluster.\n##\nadditionalPrometheusRules: []\n#  - name: my-rule-file\n#    groups:\n#      - name: my_group\n#        rules:\n#        - record: my_record\n#          expr: 100 * my_record\n\n##\nglobal:\n  rbac:\n    create: true\n    pspEnabled: true\n\n  ## Reference to one or more secrets to be used when pulling images\n  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/\n  ##\n  imagePullSecrets: []\n  # - name: \"image-pull-secret\"\n\n## Configuration for alertmanager\n## ref: https://prometheus.io/docs/alerting/alertmanager/\n##\nalertmanager:\n\n  ## Deploy alertmanager\n  ##\n  enabled: true\n\n  ## Service account for Alertmanager to use.\n  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/\n  ##\n  serviceAccount:\n    create: true\n    name: \"\"\n\n  ## Configure pod disruption budgets for Alertmanager\n  ## ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb/#specifying-a-poddisruptionbudget\n  ## This configuration is immutable once created and will require the PDB to be deleted to be changed\n  ## https://github.com/kubernetes/kubernetes/issues/45398\n  ##\n  podDisruptionBudget:\n    enabled: false\n    minAvailable: 1\n    maxUnavailable: \"\"\n\n  ## Alertmanager configuration directives\n  ## ref: https://prometheus.io/docs/alerting/configuration/#configuration-file\n  ##      https://prometheus.io/webtools/alerting/routing-tree-editor/\n  ##\n  config:\n    global:\n      resolve_timeout: 5m\n    route:\n      group_by: ['job']\n      group_wait: 30s\n      group_interval: 5m\n      repeat_interval: 12h\n      receiver: 'null'\n      routes:\n      - match:\n          alertname: Watchdog\n        receiver: 'null'\n    receivers:\n    - name: 'null'\n\n  ## Alertmanager template files to format alerts\n  ## ref: https://prometheus.io/docs/alerting/notifications/\n  ##      https://prometheus.io/docs/alerting/notification_examples/\n  ##\n  templateFiles: {}\n  #\n  # An example template:\n  #   template_1.tmpl: |-\n  #       {{ define \"cluster\" }}{{ .ExternalURL | reReplaceAll \".*alertmanager\\\\.(.*)\" \"$1\" }}{{ end }}\n  #\n  #       {{ define \"slack.myorg.text\" }}\n  #       {{- $root := . -}}\n  #       {{ range .Alerts }}\n  #         *Alert:* {{ .Annotations.summary }} - `{{ .Labels.severity }}`\n  #         *Cluster:*  {{ template \"cluster\" $root }}\n  #         *Description:* {{ .Annotations.description }}\n  #         *Graph:* \u003c{{ .GeneratorURL }}|:chart_with_upwards_trend:\u003e\n  #         *Runbook:* \u003c{{ .Annotations.runbook }}|:spiral_note_pad:\u003e\n  #         *Details:*\n  #           {{ range .Labels.SortedPairs }} • *{{ .Name }}:* `{{ .Value }}`\n  #           {{ end }}\n\n  ingress:\n    enabled: false\n\n    annotations: {}\n\n    labels: {}\n\n    ## Hosts must be provided if Ingress is enabled.\n    ##\n    hosts: []\n      # - alertmanager.domain.com\n\n    ## Paths to use for ingress rules - one path should match the alertmanagerSpec.routePrefix\n    ##\n    paths: []\n    # - /\n\n    ## TLS configuration for Alertmanager Ingress\n    ## Secret must be manually created in the namespace\n    ##\n    tls: []\n    # - secretName: alertmanager-general-tls\n    #   hosts:\n    #   - alertmanager.example.com\n\n  ## Configuration for Alertmanager service\n  ##\n  service:\n    annotations: {}\n    labels: {}\n    clusterIP: \"\"\n\n  ## Port to expose on each node\n  ## Only used if service.type is 'NodePort'\n  ##\n    nodePort: 30903\n  ## List of IP addresses at which the Prometheus server service is available\n  ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips\n  ##\n    externalIPs: []\n    loadBalancerIP: \"\"\n    loadBalancerSourceRanges: []\n    ## Service type\n    ##\n    type: ClusterIP\n\n  ## If true, create a serviceMonitor for alertmanager\n  ##\n  serviceMonitor:\n    ## Scrape interval. If not set, the Prometheus default scrape interval is used.\n    ##\n    interval: \"\"\n    selfMonitor: true\n\n  ## Settings affecting alertmanagerSpec\n  ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#alertmanagerspec\n  ##\n  alertmanagerSpec:\n    ## Standard object’s metadata. More info: https://github.com/kubernetes/community/blob/master/contributors/devel/api-conventions.md#metadata\n    ## Metadata Labels and Annotations gets propagated to the Alertmanager pods.\n    ##\n    podMetadata: {}\n\n    ## Image of Alertmanager\n    ##\n    image:\n      repository: quay.io/prometheus/alertmanager\n      tag: v0.16.1\n\n    ## Secrets is a list of Secrets in the same namespace as the Alertmanager object, which shall be mounted into the\n    ## Alertmanager Pods. The Secrets are mounted into /etc/alertmanager/secrets/.\n    ##\n    secrets: []\n\n    ## ConfigMaps is a list of ConfigMaps in the same namespace as the Alertmanager object, which shall be mounted into the Alertmanager Pods.\n    ## The ConfigMaps are mounted into /etc/alertmanager/configmaps/.\n    ##\n    configMaps: []\n\n    ## Log level for Alertmanager to be configured with.\n    ##\n    logLevel: info\n\n    ## Size is the expected size of the alertmanager cluster. The controller will eventually make the size of the\n    ## running cluster equal to the expected size.\n    replicas: 1\n\n    ## Time duration Alertmanager shall retain data for. Default is '120h', and must match the regular expression\n    ## [0-9]+(ms|s|m|h) (milliseconds seconds minutes hours).\n    ##\n    retention: 120h\n\n    ## Storage is the definition of how storage will be used by the Alertmanager instances.\n    ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/user-guides/storage.md\n    ##\n    storage: {}\n    # volumeClaimTemplate:\n    #   spec:\n    #     storageClassName: gluster\n    #     accessModes: [\"ReadWriteOnce\"]\n    #     resources:\n    #       requests:\n    #         storage: 50Gi\n    #   selector: {}\n\n\n    ## \tThe external URL the Alertmanager instances will be available under. This is necessary to generate correct URLs. This is necessary if Alertmanager is not served from root of a DNS name.\tstring\tfalse\n    ##\n    externalUrl:\n\n    ## \tThe route prefix Alertmanager registers HTTP handlers for. This is useful, if using ExternalURL and a proxy is rewriting HTTP routes of a request, and the actual ExternalURL is still true,\n    ## but the server serves requests under a different route prefix. For example for use with kubectl proxy.\n    ##\n    routePrefix: /\n\n    ## If set to true all actions on the underlying managed objects are not going to be performed, except for delete actions.\n    ##\n    paused: false\n\n    ## Define which Nodes the Pods are scheduled on.\n    ## ref: https://kubernetes.io/docs/user-guide/node-selection/\n    ##\n    nodeSelector: {}\n\n    ## Define resources requests and limits for single Pods.\n    ## ref: https://kubernetes.io/docs/user-guide/compute-resources/\n    ##\n    resources: {}\n    # requests:\n    #   memory: 400Mi\n\n    ## Pod anti-affinity can prevent the scheduler from placing Prometheus replicas on the same node.\n    ## The default value \"soft\" means that the scheduler should *prefer* to not schedule two replica pods onto the same node but no guarantee is provided.\n    ## The value \"hard\" means that the scheduler is *required* to not schedule two replica pods onto the same node.\n    ## The value \"\" will disable pod anti-affinity so that no anti-affinity rules will be configured.\n    ##\n    podAntiAffinity: \"\"\n\n    ## If anti-affinity is enabled sets the topologyKey to use for anti-affinity.\n    ## This can be changed to, for example, failure-domain.beta.kubernetes.io/zone\n    ##\n    podAntiAffinityTopologyKey: kubernetes.io/hostname\n\n    ## If specified, the pod's tolerations.\n    ## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/\n    ##\n    tolerations: []\n    # - key: \"key\"\n    #   operator: \"Equal\"\n    #   value: \"value\"\n    #   effect: \"NoSchedule\"\n\n    ## SecurityContext holds pod-level security attributes and common container settings.\n    ## This defaults to non root user with uid 1000 and gid 2000.\t*v1.PodSecurityContext\tfalse\n    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/\n    ##\n    securityContext:\n      runAsNonRoot: true\n      runAsUser: 1000\n      fsGroup: 2000\n\n    ## ListenLocal makes the Alertmanager server listen on loopback, so that it does not bind against the Pod IP.\n    ## Note this is only for the Alertmanager UI, not the gossip communication.\n    ##\n    listenLocal: false\n\n    ## Containers allows injecting additional containers. This is meant to allow adding an authentication proxy to an Alertmanager pod.\n    ##\n    containers: []\n\n    ## Priority class assigned to the Pods\n    ##\n    priorityClassName: \"\"\n\n    ## AdditionalPeers allows injecting a set of additional Alertmanagers to peer with to form a highly available cluster.\n    ##\n    additionalPeers: []\n\n## Using default values from https://github.com/helm/charts/blob/master/stable/grafana/values.yaml\n##\ngrafana:\n  enabled: true\n\n  ## Deploy default dashboards.\n  ##\n  defaultDashboardsEnabled: true\n\n  adminPassword: prom-operator\n\n  ingress:\n    ## If true, Grafana Ingress will be created\n    ##\n    enabled: false\n\n    ## Annotations for Grafana Ingress\n    ##\n    annotations: {}\n      # kubernetes.io/ingress.class: nginx\n      # kubernetes.io/tls-acme: \"true\"\n\n    ## Labels to be added to the Ingress\n    ##\n    labels: {}\n\n    ## Hostnames.\n    ## Must be provided if Ingress is enable.\n    ##\n    # hosts:\n    #   - grafana.domain.com\n    hosts: []\n\n    ## Path for grafana ingress\n    path: /\n\n    ## TLS configuration for grafana Ingress\n    ## Secret must be manually created in the namespace\n    ##\n    tls: []\n    # - secretName: grafana-general-tls\n    #   hosts:\n    #   - grafana.example.com\n\n  sidecar:\n    dashboards:\n      enabled: true\n      label: grafana_dashboard\n    datasources:\n      enabled: true\n      defaultDatasourceEnabled: true\n      label: grafana_datasource\n\n  extraConfigmapMounts: []\n  # - name: certs-configmap\n  #   mountPath: /etc/grafana/ssl/\n  #   configMap: certs-configmap\n  #   readOnly: true\n\n  ## Configure additional grafana datasources\n  ## ref: http://docs.grafana.org/administration/provisioning/#datasources\n  additionalDataSources: []\n  # - name: prometheus-sample\n  #   access: proxy\n  #   basicAuth: true\n  #   basicAuthPassword: pass\n  #   basicAuthUser: daco\n  #   editable: false\n  #   jsonData:\n  #       tlsSkipVerify: true\n  #   orgId: 1\n  #   type: prometheus\n  #   url: https://prometheus.svc:9090\n  #   version: 1\n\n  ## If true, create a serviceMonitor for grafana\n  ##\n  serviceMonitor:\n    ## Scrape interval. If not set, the Prometheus default scrape interval is used.\n    ##\n    interval: \"\"\n    selfMonitor: true\n\n## Component scraping the kube api server\n##\nkubeApiServer:\n  enabled: true\n  tlsConfig:\n    serverName: kubernetes\n    insecureSkipVerify: false\n\n  ## If your API endpoint address is not reachable (as in AKS) you can replace it with the kubernetes service\n  ##\n  relabelings: []\n  # - sourceLabels:\n  #     - __meta_kubernetes_namespace\n  #     - __meta_kubernetes_service_name\n  #     - __meta_kubernetes_endpoint_port_name\n  #   action: keep\n  #   regex: default;kubernetes;https\n  # - targetLabel: __address__\n  #   replacement: kubernetes.default.svc:443\n\n  serviceMonitor:\n    ## Scrape interval. If not set, the Prometheus default scrape interval is used.\n    ##\n    interval: \"\"\n    jobLabel: component\n    selector:\n      matchLabels:\n        component: apiserver\n        provider: kubernetes\n\n## Component scraping the kubelet and kubelet-hosted cAdvisor\n##\nkubelet:\n  enabled: true\n  namespace: kube-system\n\n  serviceMonitor:\n    ## Scrape interval. If not set, the Prometheus default scrape interval is used.\n    ##\n    interval: \"\"\n\n    ## Enable scraping the kubelet over https. For requirements to enable this see\n    ## https://github.com/coreos/prometheus-operator/issues/926\n    ##\n    https: true\n\n    ## Metric relabellings to apply to samples before ingestion\n    ##\n    cAdvisorMetricRelabelings: []\n    # - sourceLabels: [__name__, image]\n    #   separator: ;\n    #   regex: container_([a-z_]+);\n    #   replacement: $1\n    #   action: drop\n    # - sourceLabels: [__name__]\n    #   separator: ;\n    #   regex: container_(network_tcp_usage_total|network_udp_usage_total|tasks_state|cpu_load_average_10s)\n    #   replacement: $1\n    #   action: drop\n\n\n## Component scraping the kube controller manager\n##\nkubeControllerManager:\n  enabled: true\n\n  ## If your kube controller manager is not deployed as a pod, specify IPs it can be found on\n  ##\n  endpoints: []\n  # - 10.141.4.22\n  # - 10.141.4.23\n  # - 10.141.4.24\n\n  ## If using kubeControllerManager.endpoints only the port and targetPort are used\n  ##\n  service:\n    port: 10252\n    targetPort: 10252\n    selector:\n      component: kube-controller-manager\n\n  serviceMonitor:\n    ## Scrape interval. If not set, the Prometheus default scrape interval is used.\n    ##\n    interval: \"\"\n\n    ## Enable scraping kube-controller-manager over https.\n    ## Requires proper certs (not self-signed) and delegated authentication/authorization checks\n    ##\n    https: false\n\n## Component scraping coreDns. Use either this or kubeDns\n##\ncoreDns:\n  enabled: true\n  service:\n    port: 9153\n    targetPort: 9153\n    selector:\n      k8s-app: kube-dns\n  serviceMonitor:\n    ## Scrape interval. If not set, the Prometheus default scrape interval is used.\n    ##\n    interval: \"\"\n\n## Component scraping kubeDns. Use either this or coreDns\n##\nkubeDns:\n  enabled: false\n  service:\n    selector:\n      k8s-app: kube-dns\n  serviceMonitor:\n    ## Scrape interval. If not set, the Prometheus default scrape interval is used.\n    ##\n    interval: \"\"\n## Component scraping etcd\n##\nkubeEtcd:\n  enabled: true\n\n  ## If your etcd is not deployed as a pod, specify IPs it can be found on\n  ##\n  endpoints: []\n  # - 10.141.4.22\n  # - 10.141.4.23\n  # - 10.141.4.24\n\n  ## Etcd service. If using kubeEtcd.endpoints only the port and targetPort are used\n  ##\n  service:\n    port: 2379\n    targetPort: 2379\n    selector:\n      component: etcd\n\n  ## Configure secure access to the etcd cluster by loading a secret into prometheus and\n  ## specifying security configuration below. For example, with a secret named etcd-client-cert\n  ##\n  ## serviceMonitor:\n  ##   scheme: https\n  ##   insecureSkipVerify: false\n  ##   serverName: localhost\n  ##   caFile: /etc/prometheus/secrets/etcd-client-cert/etcd-ca\n  ##   certFile: /etc/prometheus/secrets/etcd-client-cert/etcd-client\n  ##   keyFile: /etc/prometheus/secrets/etcd-client-cert/etcd-client-key\n  ##\n  serviceMonitor:\n    ## Scrape interval. If not set, the Prometheus default scrape interval is used.\n    ##\n    interval: \"\"\n    scheme: http\n    insecureSkipVerify: false\n    serverName: \"\"\n    caFile: \"\"\n    certFile: \"\"\n    keyFile: \"\"\n\n\n## Component scraping kube scheduler\n##\nkubeScheduler:\n  enabled: true\n\n  ## If your kube scheduler is not deployed as a pod, specify IPs it can be found on\n  ##\n  endpoints: []\n  # - 10.141.4.22\n  # - 10.141.4.23\n  # - 10.141.4.24\n\n  ## If using kubeScheduler.endpoints only the port and targetPort are used\n  ##\n  service:\n    port: 10251\n    targetPort: 10251\n    selector:\n      component: kube-scheduler\n\n  serviceMonitor:\n    ## Scrape interval. If not set, the Prometheus default scrape interval is used.\n    ##\n    interval: \"\"\n    ## Enable scraping kube-controller-manager over https.\n    ## Requires proper certs (not self-signed) and delegated authentication/authorization checks\n    ##\n    https: false\n\n## Component scraping kube state metrics\n##\nkubeStateMetrics:\n  enabled: true\n  serviceMonitor:\n    ## Scrape interval. If not set, the Prometheus default scrape interval is used.\n    ##\n    interval: \"\"\n\n    ## \tmetric relabel configs to apply to samples before ingestion.\n    ##\n    metricRelabelings: []\n    # - action: keep\n    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'\n    #   sourceLabels: [__name__]\n\n## Configuration for kube-state-metrics subchart\n##\nkube-state-metrics:\n  rbac:\n    create: true\n  podSecurityPolicy:\n    enabled: true\n\n## Deploy node exporter as a daemonset to all nodes\n##\nnodeExporter:\n  enabled: true\n\n  ## Use the value configured in prometheus-node-exporter.podLabels\n  ##\n  jobLabel: jobLabel\n\n  serviceMonitor:\n    ## Scrape interval. If not set, the Prometheus default scrape interval is used.\n    ##\n    interval: \"\"\n\n    ## \tmetric relabel configs to apply to samples before ingestion.\n    ##\n    metricRelabelings: []\n    # - sourceLabels: [__name__]\n    #   separator: ;\n    #   regex: ^node_mountstats_nfs_(event|operations|transport)_.+\n    #   replacement: $1\n    #   action: drop\n\n## Configuration for prometheus-node-exporter subchart\n##\nprometheus-node-exporter:\n  podLabels:\n    ## Add the 'node-exporter' label to be used by serviceMonitor to match standard common usage in rules and grafana dashboards\n    ##\n    jobLabel: node-exporter\n  extraArgs:\n    - --collector.filesystem.ignored-mount-points=^/(dev|proc|sys|var/lib/docker/.+)($|/)\n    - --collector.filesystem.ignored-fs-types=^(autofs|binfmt_misc|cgroup|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|mqueue|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|sysfs|tracefs)$\n\n## Manages Prometheus and Alertmanager components\n##\nprometheusOperator:\n  enabled: true\n\n  ## Service account for Alertmanager to use.\n  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/\n  ##\n  serviceAccount:\n    create: true\n    name: \"\"\n\n  ## Configuration for Prometheus operator service\n  ##\n  service:\n    annotations: {}\n    labels: {}\n    clusterIP: \"\"\n\n  ## Port to expose on each node\n  ## Only used if service.type is 'NodePort'\n  ##\n    nodePort: 30080\n\n  ## Additional ports to open for Prometheus service\n  ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#multi-port-services\n  ##\n    additionalPorts: []\n    #  - name: thanos-cluster\n    #    port: 10900\n    #    nodePort: 30111\n\n  ## Loadbalancer IP\n  ## Only use if service.type is \"loadbalancer\"\n  ##\n    loadBalancerIP: \"\"\n    loadBalancerSourceRanges: []\n\n  ## Service type\n  ## NodepPort, ClusterIP, loadbalancer\n  ##\n    type: ClusterIP\n\n    ## List of IP addresses at which the Prometheus server service is available\n    ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips\n    ##\n    externalIPs: []\n\n  ## Deploy CRDs used by Prometheus Operator.\n  ##\n  createCustomResource: true\n\n  ## Customize CRDs API Group\n  crdApiGroup: monitoring.coreos.com\n\n  ## Attempt to clean up CRDs created by Prometheus Operator.\n  ##\n  cleanupCustomResource: false\n\n  ## Labels to add to the operator pod\n  ##\n  podLabels: {}\n\n  ## Annotations to add to the operator pod\n  ##\n  podAnnotations: {}\n\n  ## Assign a PriorityClassName to pods if set\n  # priorityClassName: \"\"\n\n  ## Define Log Format\n  # Use logfmt (default) or json-formatted logging\n  # logFormat: logfmt\n\n  ## Decrease log verbosity to errors only\n  # logLevel: error\n\n  ## If true, the operator will create and maintain a service for scraping kubelets\n  ## ref: https://github.com/coreos/prometheus-operator/blob/master/helm/prometheus-operator/README.md\n  ##\n  kubeletService:\n    enabled: true\n    namespace: kube-system\n\n  ## Create a servicemonitor for the operator\n  ##\n  serviceMonitor:\n    ## Scrape interval. If not set, the Prometheus default scrape interval is used.\n    ##\n    interval: \"\"\n    selfMonitor: true\n\n  ## Resource limits \u0026 requests\n  ##\n  resources: {}\n  # limits:\n  #   cpu: 200m\n  #   memory: 200Mi\n  # requests:\n  #   cpu: 100m\n  #   memory: 100Mi\n\n  ## Define which Nodes the Pods are scheduled on.\n  ## ref: https://kubernetes.io/docs/user-guide/node-selection/\n  ##\n  nodeSelector: {}\n\n  ## Tolerations for use with node taints\n  ## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/\n  ##\n  tolerations: []\n  # - key: \"key\"\n  #   operator: \"Equal\"\n  #   value: \"value\"\n  #   effect: \"NoSchedule\"\n\n  ## Assign the prometheus operator to run on specific nodes\n  ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/\n  ##\n  affinity: {}\n  # requiredDuringSchedulingIgnoredDuringExecution:\n  #   nodeSelectorTerms:\n  #   - matchExpressions:\n  #     - key: kubernetes.io/e2e-az-name\n  #       operator: In\n  #       values:\n  #       - e2e-az1\n  #       - e2e-az2\n\n  securityContext:\n    runAsNonRoot: true\n    runAsUser: 65534\n\n  ## Prometheus-operator image\n  ##\n  image:\n    repository: quay.io/coreos/prometheus-operator\n    tag: v0.29.0\n    pullPolicy: IfNotPresent\n\n  ## Configmap-reload image to use for reloading configmaps\n  ##\n  configmapReloadImage:\n    repository: quay.io/coreos/configmap-reload\n    tag: v0.0.1\n\n  ## Prometheus-config-reloader image to use for config and rule reloading\n  ##\n  prometheusConfigReloaderImage:\n    repository: quay.io/coreos/prometheus-config-reloader\n    tag: v0.29.0\n\n  ## Set the prometheus config reloader side-car CPU limit. If unset, uses the prometheus-operator project default\n  ##\n  # configReloaderCpu: 100m\n\n  ## Set the prometheus config reloader side-car memory limit. If unset, uses the prometheus-operator project default\n  ##\n  # configReloaderMemory: 25Mi\n\n  ## Hyperkube image to use when cleaning up\n  ##\n  hyperkubeImage:\n    repository: k8s.gcr.io/hyperkube\n    tag: v1.12.1\n    pullPolicy: IfNotPresent\n\n## Deploy a Prometheus instance\n##\nprometheus:\n\n  enabled: true\n\n  ## Service account for Prometheuses to use.\n  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/\n  ##\n  serviceAccount:\n    create: true\n    name: \"\"\n\n  ## Configuration for Prometheus service\n  ##\n  service:\n    annotations: {}\n    labels: {}\n    clusterIP: \"\"\n\n\n    ## To be used with a proxy extraContainer port\n    targetPort: 9090\n\n    ## List of IP addresses at which the Prometheus server service is available\n    ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips\n    ##\n    externalIPs: []\n\n    ## Port to expose on each node\n    ## Only used if service.type is 'NodePort'\n    ##\n    nodePort: 30090\n\n    ## Loadbalancer IP\n    ## Only use if service.type is \"loadbalancer\"\n    loadBalancerIP: \"\"\n    loadBalancerSourceRanges: []\n    ## Service type\n    ##\n    type: ClusterIP\n\n    sessionAffinity: \"\"\n\n  rbac:\n    ## Create role bindings in the specified namespaces, to allow Prometheus monitoring\n    ## a role binding in the release namespace will always be created.\n    ##\n    roleNamespaces:\n      - kube-system\n\n  ## Configure pod disruption budgets for Prometheus\n  ## ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb/#specifying-a-poddisruptionbudget\n  ## This configuration is immutable once created and will require the PDB to be deleted to be changed\n  ## https://github.com/kubernetes/kubernetes/issues/45398\n  ##\n  podDisruptionBudget:\n    enabled: false\n    minAvailable: 1\n    maxUnavailable: \"\"\n\n  ingress:\n    enabled: false\n    annotations: {}\n    labels: {}\n\n    ## Hostnames.\n    ## Must be provided if Ingress is enabled.\n    ##\n    # hosts:\n    #   - prometheus.domain.com\n    hosts: []\n\n    ## Paths to use for ingress rules - one path should match the prometheusSpec.routePrefix\n    ##\n    paths: []\n    # - /\n\n    ## TLS configuration for Prometheus Ingress\n    ## Secret must be manually created in the namespace\n    ##\n    tls: []\n      # - secretName: prometheus-general-tls\n      #   hosts:\n      #     - prometheus.example.com\n\n  serviceMonitor:\n    ## Scrape interval. If not set, the Prometheus default scrape interval is used.\n    ##\n    interval: \"\"\n    selfMonitor: true\n\n  ## Settings affecting prometheusSpec\n  ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#prometheusspec\n  ##\n  prometheusSpec:\n\n    ## Interval between consecutive scrapes.\n    ##\n    scrapeInterval: \"\"\n\n    ## Interval between consecutive evaluations.\n    ##\n    evaluationInterval: \"\"\n\n    ## ListenLocal makes the Prometheus server listen on loopback, so that it does not bind against the Pod IP.\n    ##\n    listenLocal: false\n\n    ## Image of Prometheus.\n    ##\n    image:\n      repository: quay.io/prometheus/prometheus\n      tag: v2.7.1\n\n    ## Tolerations for use with node taints\n    ## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/\n    ##\n    tolerations: []\n    #  - key: \"key\"\n    #    operator: \"Equal\"\n    #    value: \"value\"\n    #    effect: \"NoSchedule\"\n\n    ## Alertmanagers to which alerts will be sent\n    ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#alertmanagerendpoints\n    ##\n    ## Default configuration will connect to the alertmanager deployed as part of this release\n    ##\n    alertingEndpoints: []\n    # - name: \"\"\n    #   namespace: \"\"\n    #   port: http\n    #   scheme: http\n\n    ## External labels to add to any time series or alerts when communicating with external systems\n    ##\n    externalLabels: {}\n\n    ## External URL at which Prometheus will be reachable.\n    ##\n    externalUrl: \"\"\n\n    ## Define which Nodes the Pods are scheduled on.\n    ## ref: https://kubernetes.io/docs/user-guide/node-selection/\n    ##\n    nodeSelector: {}\n\n    ## Secrets is a list of Secrets in the same namespace as the Prometheus object, which shall be mounted into the Prometheus Pods.\n    ## The Secrets are mounted into /etc/prometheus/secrets/. Secrets changes after initial creation of a Prometheus object are not\n    ## reflected in the running Pods. To change the secrets mounted into the Prometheus Pods, the object must be deleted and recreated\n    ## with the new list of secrets.\n    ##\n    secrets: []\n\n    ## ConfigMaps is a list of ConfigMaps in the same namespace as the Prometheus object, which shall be mounted into the Prometheus Pods.\n    ## The ConfigMaps are mounted into /etc/prometheus/configmaps/.\n    ##\n    configMaps: []\n\n    ## QuerySpec defines the query command line flags when starting Prometheus.\n    ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#queryspec\n    ##\n    query: {}\n\n    ## Namespaces to be selected for PrometheusRules discovery.\n    ## If nil, select own namespace. Namespaces to be selected for ServiceMonitor discovery.\n    ## See https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#namespaceselector for usage\n    ##\n    ruleNamespaceSelector: {}\n\n    ## If true, a nil or {} value for prometheus.prometheusSpec.ruleSelector will cause the\n    ## prometheus resource to be created with selectors based on values in the helm deployment,\n    ## which will also match the PrometheusRule resources created\n    ##\n    ruleSelectorNilUsesHelmValues: true\n\n    ## PrometheusRules to be selected for target discovery.\n    ## If {}, select all ServiceMonitors\n    ##\n    ruleSelector: {}\n    ## Example which select all prometheusrules resources\n    ## with label \"prometheus\" with values any of \"example-rules\" or \"example-rules-2\"\n    # ruleSelector:\n    #   matchExpressions:\n    #     - key: prometheus\n    #       operator: In\n    #       values:\n    #         - example-rules\n    #         - example-rules-2\n    #\n    ## Example which select all prometheusrules resources with label \"role\" set to \"example-rules\"\n    # ruleSelector:\n    #   matchLabels:\n    #     role: example-rules\n\n    ## If true, a nil or {} value for prometheus.prometheusSpec.serviceMonitorSelector will cause the\n    ## prometheus resource to be created with selectors based on values in the helm deployment,\n    ## which will also match the servicemonitors created\n    ##\n    serviceMonitorSelectorNilUsesHelmValues: true\n\n    ## ServiceMonitors to be selected for target discovery.\n    ## If {}, select all ServiceMonitors\n    ##\n    serviceMonitorSelector: {}\n    ## Example which selects ServiceMonitors with label \"prometheus\" set to \"somelabel\"\n    # serviceMonitorSelector:\n    #   matchLabels:\n    #     prometheus: somelabel\n\n    ## Namespaces to be selected for ServiceMonitor discovery.\n    ## See https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#namespaceselector for usage\n    ##\n    serviceMonitorNamespaceSelector: {}\n\n    ## How long to retain metrics\n    ##\n    retention: 10d\n\n    ## If true, the Operator won't process any Prometheus configuration changes\n    ##\n    paused: false\n\n    ## Number of Prometheus replicas desired\n    ##\n    replicas: 1\n\n    ## Log level for Prometheus be configured in\n    ##\n    logLevel: info\n\n    ## Prefix used to register routes, overriding externalUrl route.\n    ## Useful for proxies that rewrite URLs.\n    ##\n    routePrefix: /\n\n    ## Standard object’s metadata. More info: https://github.com/kubernetes/community/blob/master/contributors/devel/api-conventions.md#metadata\n    ## Metadata Labels and Annotations gets propagated to the prometheus pods.\n    ##\n    podMetadata: {}\n    # labels:\n    #   app: prometheus\n    #   k8s-app: prometheus\n\n    ## Pod anti-affinity can prevent the scheduler from placing Prometheus replicas on the same node.\n    ## The default value \"soft\" means that the scheduler should *prefer* to not schedule two replica pods onto the same node but no guarantee is provided.\n    ## The value \"hard\" means that the scheduler is *required* to not schedule two replica pods onto the same node.\n    ## The value \"\" will disable pod anti-affinity so that no anti-affinity rules will be configured.\n    podAntiAffinity: \"\"\n\n    ## If anti-affinity is enabled sets the topologyKey to use for anti-affinity.\n    ## This can be changed to, for example, failure-domain.beta.kubernetes.io/zone\n    ##\n    podAntiAffinityTopologyKey: kubernetes.io/hostname\n\n    ## The remote_read spec configuration for Prometheus.\n    ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#remotereadspec\n    remoteRead: []\n    # - url: http://remote1/read\n\n    ## The remote_write spec configuration for Prometheus.\n    ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#remotewritespec\n    remoteWrite: []\n    # - url: http://remote1/push\n\n    ## Resource limits \u0026 requests\n    ##\n    resources: {}\n    # requests:\n    #   memory: 400Mi\n\n    ## Prometheus StorageSpec for persistent data\n    ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/user-guides/storage.md\n    ##\n    storageSpec: {}\n    #  volumeClaimTemplate:\n    #    spec:\n    #      storageClassName: gluster\n    #      accessModes: [\"ReadWriteOnce\"]\n    #      resources:\n    #        requests:\n    #          storage: 50Gi\n    #    selector: {}\n\n    ## AdditionalScrapeConfigs allows specifying additional Prometheus scrape configurations. Scrape configurations\n    ## are appended to the configurations generated by the Prometheus Operator. Job configurations must have the form\n    ## as specified in the official Prometheus documentation:\n    ## https://prometheus.io/docs/prometheus/latest/configuration/configuration/#\u003cscrape_config\u003e. As scrape configs are\n    ## appended, the user is responsible to make sure it is valid. Note that using this feature may expose the possibility\n    ## to break upgrades of Prometheus. It is advised to review Prometheus release notes to ensure that no incompatible\n    ## scrape configs are going to break Prometheus after the upgrade.\n    ##\n    ## The scrape configuraiton example below will find master nodes, provided they have the name .*mst.*, relabel the\n    ## port to 2379 and allow etcd scraping provided it is running on all Kubernetes master nodes\n    ##\n    additionalScrapeConfigs: []\n    # - job_name: kube-etcd\n    #   kubernetes_sd_configs:\n    #     - role: node\n    #   scheme: https\n    #   tls_config:\n    #     ca_file:   /etc/prometheus/secrets/etcd-client-cert/etcd-ca\n    #     cert_file: /etc/prometheus/secrets/etcd-client-cert/etcd-client\n    #     key_file:  /etc/prometheus/secrets/etcd-client-cert/etcd-client-key\n    #   relabel_configs:\n    #   - action: labelmap\n    #     regex: __meta_kubernetes_node_label_(.+)\n    #   - source_labels: [__address__]\n    #     action: replace\n    #     target_label: __address__\n    #     regex: ([^:;]+):(\\d+)\n    #     replacement: ${1}:2379\n    #   - source_labels: [__meta_kubernetes_node_name]\n    #     action: keep\n    #     regex: .*mst.*\n    #   - source_labels: [__meta_kubernetes_node_name]\n    #     action: replace\n    #     target_label: node\n    #     regex: (.*)\n    #     replacement: ${1}\n    #   metric_relabel_configs:\n    #   - regex: (kubernetes_io_hostname|failure_domain_beta_kubernetes_io_region|beta_kubernetes_io_os|beta_kubernetes_io_arch|beta_kubernetes_io_instance_type|failure_domain_beta_kubernetes_io_zone)\n    #     action: labeldrop\n\n\n    ## AdditionalAlertManagerConfigs allows for manual configuration of alertmanager jobs in the form as specified\n    ## in the official Prometheus documentation https://prometheus.io/docs/prometheus/latest/configuration/configuration/#\u003calertmanager_config\u003e.\n    ## AlertManager configurations specified are appended to the configurations generated by the Prometheus Operator.\n    ## As AlertManager configs are appended, the user is responsible to make sure it is valid. Note that using this\n    ## feature may expose the possibility to break upgrades of Prometheus. It is advised to review Prometheus release\n    ## notes to ensure that no incompatible AlertManager configs are going to break Prometheus after the upgrade.\n    ##\n    additionalAlertManagerConfigs: []\n    # - consul_sd_configs:\n    #   - server: consul.dev.test:8500\n    #     scheme: http\n    #     datacenter: dev\n    #     tag_separator: ','\n    #     services:\n    #       - metrics-prometheus-alertmanager\n\n    ## AdditionalAlertRelabelConfigs allows specifying Prometheus alert relabel configurations. Alert relabel configurations specified are appended\n    ## to the configurations generated by the Prometheus Operator. Alert relabel configurations specified must have the form as specified in the\n    ## official Prometheus documentation: https://prometheus.io/docs/prometheus/latest/configuration/configuration/#alert_relabel_configs.\n    ## As alert relabel configs are appended, the user is responsible to make sure it is valid. Note that using this feature may expose the\n    ## possibility to break upgrades of Prometheus. It is advised to review Prometheus release notes to ensure that no incompatible alert relabel\n    ## configs are going to break Prometheus after the upgrade.\n    ##\n    additionalAlertRelabelConfigs: []\n    # - separator: ;\n    #   regex: prometheus_replica\n    #   replacement: $1\n    #   action: labeldrop\n\n    ## SecurityContext holds pod-level security attributes and common container settings.\n    ## This defaults to non root user with uid 1000 and gid 2000.\n    ## https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md\n    ##\n    securityContext:\n      runAsNonRoot: true\n      runAsUser: 1000\n      fsGroup: 2000\n\n    ## \tPriority class assigned to the Pods\n    ##\n    priorityClassName: \"\"\n\n    ## Thanos configuration allows configuring various aspects of a Prometheus server in a Thanos environment.\n    ## This section is experimental, it may change significantly without deprecation notice in any release.\n    ## This is experimental and may change significantly without backward compatibility in any release.\n    ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#thanosspec\n    ##\n    thanos: {}\n\n    ## Containers allows injecting additional containers. This is meant to allow adding an authentication proxy to a Prometheus pod.\n    ##  if using proxy extraContainer  update targetPort with proxy container port\n    containers: []\n\n    ## Enable additional scrape configs that are managed externally to this chart. Note that the prometheus\n    ## will fail to provision if the correct secret does not exist.\n    ##\n    additionalScrapeConfigsExternal: false\n\n  additionalServiceMonitors: []\n  ## Name of the ServiceMonitor to create\n  ##\n  # - name: \"\"\n\n    ## Additional labels to set used for the ServiceMonitorSelector. Together with standard labels from\n    ## the chart\n    ##\n    # additionalLabels: {}\n\n    ## Service label for use in assembling a job name of the form \u003clabel value\u003e-\u003cport\u003e\n    ## If no label is specified, the service name is used.\n    ##\n    # jobLabel: \"\"\n\n    ## Label selector for services to which this ServiceMonitor applies\n    ##\n    # selector: {}\n\n    ## Namespaces from which services are selected\n    ##\n    # namespaceSelector:\n      ## Match any namespace\n      ##\n      # any: false\n\n      ## Explicit list of namespace names to select\n      ##\n      # matchNames: []\n\n    ## Endpoints of the selected service to be monitored\n    ##\n    # endpoints: []\n      ## Name of the endpoint's service port\n      ## Mutually exclusive with targetPort\n      # - port: \"\"\n\n      ## Name or number of the endpoint's target port\n      ## Mutually exclusive with port\n      # - targetPort: \"\"\n\n      ## File containing bearer token to be used when scraping targets\n      ##\n      #   bearerTokenFile: \"\"\n\n      ## Interval at which metrics should be scraped\n      ##\n      #   interval: 30s\n\n      ## HTTP path to scrape for metrics\n      ##\n      #   path: /metrics\n\n      ## HTTP scheme to use for scraping\n      ##\n      #   scheme: http\n\n      ## TLS configuration to use when scraping the endpoint\n      ##\n      #   tlsConfig:\n\n          ## Path to the CA file\n          ##\n          # caFile: \"\"\n\n          ## Path to client certificate file\n          ##\n          # certFile: \"\"\n\n          ## Skip certificate verification\n          ##\n          # insecureSkipVerify: false\n\n          ## Path to client key file\n          ##\n          # keyFile: \"\"\n\n          ## Server name used to verify host name\n          ##\n          # serverName: \"\"\n",
    "kustomize": {
      "overlays": {
        "ship": {
          "patches": {
            "/prometheus/prometheus.yaml": "--- \napiVersion: monitoring.coreos.com/v1\nkind: Prometheus\nmetadata:\n  name: prometheus-operator-prometheus\nspec:\n  serviceMonitorSelector:\n    matchLabels:\n      app: etcd"
          }
        }
      }
    },
    "upstream": "github.com/helm/charts/stable/prometheus-operator",
    "metadata": {
      "applicationType": "helm",
      "icon": "https://raw.githubusercontent.com/prometheus/prometheus.github.io/master/assets/prometheus_logo-cb55bb5c346.png",
      "name": "prometheus-operator",
      "releaseNotes": "[stable/prometheus-operator] Allow metricRelabelings in default kube-state-metrics exporter (#12960)\n\n* [stable/prometheus-operator] metricRelabelings in kube-state exporter\n\nSigned-off-by: Marcos Estevez \u003cmarcos.stvz@gmail.com\u003e\n\n* Adding kubeStateMetrics.serviceMonitor.metricRelabelings to ci\n\nSigned-off-by: Marcos Estevez \u003cmarcos.stvz@gmail.com\u003e",
      "version": "5.0.11"
    },
    "contentSHA": "a9936758bbd847f89aa671dae50c88dba280ffdb598d3916dc92263fb7ff2001",
    "lifecycle": {
      "stepsCompleted": {
        "intro": true,
        "kustomize": true,
        "kustomize-intro": true,
        "outro": true,
        "render": true,
        "values": true
      }
    }
  }
}